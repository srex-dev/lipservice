"""
Intelligent Log Analysis Engine

This module provides AI-powered log analysis including semantic clustering,
temporal correlation, and intelligent insights generation.
"""

import asyncio
import time
import uuid
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Tuple

import numpy as np
from sklearn.cluster import HDBSCAN
from sklearn.metrics.pairwise import cosine_similarity


class LogContext:
    """Simple log context for intelligent analysis"""
    
    def __init__(self, **kwargs):
        self.data = kwargs
    
    def __repr__(self):
        return f"LogContext({self.data})"


class LLMProvider:
    """LLM provider for generating embeddings and text analysis"""
    
    def __init__(self, provider: str = "openai"):
        self.provider = provider
        self._client = None
    
    async def generate_embeddings(self, texts: List[str]) -> np.ndarray:
        """Generate embeddings for a list of texts"""
        # For now, use a simple hash-based embedding
        # In production, this would use OpenAI, Cohere, or local models
        embeddings = []
        for text in texts:
            # Simple hash-based embedding (replace with real LLM)
            embedding = self._hash_to_embedding(text)
            embeddings.append(embedding)
        
        return np.array(embeddings)
    
    async def generate(self, prompt: str) -> str:
        """Generate text using LLM"""
        # For now, return a mock response
        # In production, this would use real LLM APIs
        return f"LLM Response for: {prompt[:50]}..."
    
    def _hash_to_embedding(self, text: str) -> List[float]:
        """Convert text to embedding using hash (placeholder)"""
        import hashlib
        
        # Create a simple hash-based embedding
        hash_obj = hashlib.md5(text.encode())
        hash_bytes = hash_obj.digest()
        
        # Convert to 384-dimensional embedding (typical for sentence transformers)
        embedding = []
        for i in range(384):
            byte_idx = i % len(hash_bytes)
            embedding.append(float(hash_bytes[byte_idx]) / 255.0)
        
        return embedding


class LogEntry:
    """Enhanced log entry with metadata"""
    
    def __init__(self, id: str = None, timestamp: datetime = None, level: str = None,
                 message: str = None, service_name: str = None, context: LogContext = None,
                 metadata: Dict[str, Any] = None):
        self.id = id or str(uuid.uuid4())
        self.timestamp = timestamp or datetime.utcnow()
        self.level = level or "INFO"
        self.message = message or ""
        self.service_name = service_name or "unknown"
        self.context = context
        self.metadata = metadata or {}


class Entity:
    """Extracted entity from log"""
    
    def __init__(self, name: str, type: str, value: str, confidence: float = 1.0):
        self.name = name
        self.type = type
        self.value = value
        self.confidence = confidence


class IntelligentTag:
    """Intelligent tag generated by LLM"""
    
    def __init__(self, name: str, category: str, value: str, confidence: float = 1.0):
        self.name = name
        self.category = category
        self.value = value
        self.confidence = confidence


class EnrichedLogEntry:
    """Log entry enriched with intelligent metadata"""
    
    def __init__(self, original_log: LogEntry, entities: List[Entity] = None,
                 intelligent_tags: List[IntelligentTag] = None, importance_score: float = 0.5,
                 semantic_content: str = None, enrichment_metadata: Dict[str, Any] = None):
        self.original_log = original_log
        self.entities = entities or []
        self.intelligent_tags = intelligent_tags or []
        self.importance_score = importance_score
        self.semantic_content = semantic_content or self._extract_semantic_content()
        self.enrichment_metadata = enrichment_metadata or {}
    
    def _extract_semantic_content(self) -> str:
        """Extract semantic content from log"""
        return f"{self.original_log.level}: {self.original_log.message}"


class LogCluster:
    """Semantic cluster of logs"""
    
    def __init__(self, id: str = None, name: str = None, description: str = None,
                 logs: List[EnrichedLogEntry] = None, centroid_embedding: List[float] = None,
                 semantic_summary: str = None, cluster_metadata: Dict[str, Any] = None):
        self.id = id or str(uuid.uuid4())
        self.name = name or f"Cluster {self.id[:8]}"
        self.description = description or ""
        self.logs = logs or []
        self.centroid_embedding = centroid_embedding or []
        self.semantic_summary = semantic_summary or ""
        self.cluster_metadata = cluster_metadata or {}


class CorrelationPattern:
    """Temporal correlation pattern"""
    
    def __init__(self, id: str = None, pattern_type: str = None, events: List[EnrichedLogEntry] = None,
                 time_window: timedelta = None, correlation_strength: float = 0.0,
                 description: str = None, confidence: float = 0.0):
        self.id = id or str(uuid.uuid4())
        self.pattern_type = pattern_type or "sequence"
        self.events = events or []
        self.time_window = time_window or timedelta(minutes=5)
        self.correlation_strength = correlation_strength
        self.description = description or ""
        self.confidence = confidence


class LogInsight:
    """Intelligent insight generated from log analysis"""
    
    def __init__(self, id: str = None, type: str = None, title: str = None,
                 description: str = None, severity: str = "medium", actionable: bool = True,
                 recommended_actions: List[str] = None, confidence: float = 0.5,
                 generated_at: datetime = None):
        self.id = id or str(uuid.uuid4())
        self.type = type or "general"
        self.title = title or "Log Insight"
        self.description = description or ""
        self.severity = severity
        self.actionable = actionable
        self.recommended_actions = recommended_actions or []
        self.confidence = confidence
        self.generated_at = generated_at or datetime.utcnow()


class SemanticLogClusterer:
    """AI-powered log clustering using semantic similarity"""
    
    def __init__(self, llm_provider: str = "openai"):
        self.llm = LLMProvider(llm_provider)
        self.embeddings_cache = {}
        self.cluster_centroids = {}
    
    async def cluster_logs(self, logs: List[LogEntry], threshold: float = 0.7) -> List[LogCluster]:
        """Group logs by semantic similarity"""
        
        # 1. Generate embeddings for each log
        embeddings = await self._generate_embeddings(logs)
        
        # 2. Apply semantic clustering
        cluster_labels = self._semantic_clustering(embeddings, threshold)
        
        # 3. Create clusters
        clusters = self._create_clusters(logs, embeddings, cluster_labels)
        
        # 4. Generate cluster summaries using LLM
        enriched_clusters = await self._enrich_clusters(clusters)
        
        return enriched_clusters
    
    async def _generate_embeddings(self, logs: List[LogEntry]) -> np.ndarray:
        """Generate semantic embeddings for logs"""
        texts = [self._extract_semantic_content(log) for log in logs]
        return await self.llm.generate_embeddings(texts)
    
    def _extract_semantic_content(self, log: LogEntry) -> str:
        """Extract semantic content, removing noise"""
        # Remove timestamps, IDs, IPs, etc. and keep semantic meaning
        content = f"{log.level}: {log.message}"
        if log.context:
            content += f" | Context: {log.context}"
        return content
    
    def _semantic_clustering(self, embeddings: np.ndarray, threshold: float) -> List[int]:
        """Apply semantic clustering using HDBSCAN"""
        if len(embeddings) < 2:
            return [0] * len(embeddings)
        
        # Use HDBSCAN for clustering
        clusterer = HDBSCAN(min_cluster_size=2, min_samples=1)
        cluster_labels = clusterer.fit_predict(embeddings)
        
        return cluster_labels.tolist()
    
    def _create_clusters(self, logs: List[LogEntry], embeddings: np.ndarray, 
                        cluster_labels: List[int]) -> List[LogCluster]:
        """Create cluster objects from clustering results"""
        clusters = {}
        
        for i, (log, embedding, label) in enumerate(zip(logs, embeddings, cluster_labels)):
            if label not in clusters:
                clusters[label] = {
                    'logs': [],
                    'embeddings': [],
                    'centroid': None
                }
            
            clusters[label]['logs'].append(log)
            clusters[label]['embeddings'].append(embedding)
        
        # Calculate centroids
        cluster_objects = []
        for label, cluster_data in clusters.items():
            logs = cluster_data['logs']
            embeddings = np.array(cluster_data['embeddings'])
            centroid = np.mean(embeddings, axis=0)
            
            cluster = LogCluster(
                logs=logs,
                centroid_embedding=centroid.tolist(),
                name=f"Cluster {label}" if label >= 0 else "Noise"
            )
            cluster_objects.append(cluster)
        
        return cluster_objects
    
    async def _enrich_clusters(self, clusters: List[LogCluster]) -> List[LogCluster]:
        """Enrich clusters with LLM-generated summaries"""
        enriched_clusters = []
        
        for cluster in clusters:
            if len(cluster.logs) > 0:
                # Generate semantic summary
                summary = await self._generate_cluster_summary(cluster)
                cluster.semantic_summary = summary
                
                # Generate cluster name
                name = await self._generate_cluster_name(cluster)
                cluster.name = name
                
                # Generate description
                description = await self._generate_cluster_description(cluster)
                cluster.description = description
            
            enriched_clusters.append(cluster)
        
        return enriched_clusters
    
    async def _generate_cluster_summary(self, cluster: LogCluster) -> str:
        """Generate semantic summary for cluster"""
        if len(cluster.logs) == 0:
            return "Empty cluster"
        
        # Create summary from log messages
        messages = [log.message for log in cluster.logs[:5]]  # Use first 5 messages
        summary_text = " | ".join(messages)
        
        prompt = f"""
        Analyze these log messages and provide a semantic summary:
        
        Messages: {summary_text}
        
        Provide a concise summary of what this cluster represents.
        """
        
        return await self.llm.generate(prompt)
    
    async def _generate_cluster_name(self, cluster: LogCluster) -> str:
        """Generate descriptive name for cluster"""
        if len(cluster.logs) == 0:
            return "Empty Cluster"
        
        # Analyze log levels and messages
        levels = [log.level for log in cluster.logs]
        most_common_level = max(set(levels), key=levels.count)
        
        # Extract common patterns
        messages = [log.message for log in cluster.logs[:3]]
        common_words = self._extract_common_words(messages)
        
        if common_words:
            return f"{most_common_level} - {common_words[0]}"
        else:
            return f"{most_common_level} Cluster"
    
    async def _generate_cluster_description(self, cluster: LogCluster) -> str:
        """Generate detailed description for cluster"""
        if len(cluster.logs) == 0:
            return "No logs in this cluster"
        
        # Analyze cluster characteristics
        levels = [log.level for log in cluster.logs]
        services = [log.service_name for log in cluster.logs]
        
        level_dist = {level: levels.count(level) for level in set(levels)}
        service_dist = {service: services.count(service) for service in set(services)}
        
        description = f"Cluster containing {len(cluster.logs)} logs"
        description += f" with levels: {level_dist}"
        description += f" from services: {service_dist}"
        
        return description
    
    def _extract_common_words(self, messages: List[str]) -> List[str]:
        """Extract common words from messages"""
        import re
        from collections import Counter
        
        # Extract words from messages
        all_words = []
        for message in messages:
            words = re.findall(r'\b\w+\b', message.lower())
            all_words.extend(words)
        
        # Count word frequency
        word_counts = Counter(all_words)
        
        # Return most common words (excluding common stop words)
        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}
        common_words = [word for word, count in word_counts.most_common(5) 
                       if word not in stop_words and len(word) > 2]
        
        return common_words


class LogMetadataExtractor:
    """Extract and enrich log metadata using LLMs"""
    
    def __init__(self, llm_provider: str = "openai"):
        self.llm = LLMProvider(llm_provider)
    
    async def enrich_log(self, log: LogEntry) -> EnrichedLogEntry:
        """Extract semantic metadata from logs"""
        
        # 1. Extract entities
        entities = await self._extract_entities(log)
        
        # 2. Generate intelligent tags
        tags = await self._generate_intelligent_tags(log, entities)
        
        # 3. Calculate importance score
        importance = await self._calculate_importance(log, entities)
        
        # 4. Extract semantic content
        semantic_content = self._extract_semantic_content(log)
        
        return EnrichedLogEntry(
            original_log=log,
            entities=entities,
            intelligent_tags=tags,
            importance_score=importance,
            semantic_content=semantic_content,
            enrichment_metadata={
                'enriched_at': datetime.utcnow(),
                'extractor_version': '1.0'
            }
        )
    
    async def _extract_entities(self, log: LogEntry) -> List[Entity]:
        """Extract entities like users, services, errors, etc."""
        prompt = f"""
        Extract entities from this log entry:
        "{log.message}"
        
        Return entities in this format:
        - user_ids: List of user identifiers
        - service_names: List of service names  
        - error_types: List of error types
        - resource_names: List of resources (DB, API, etc.)
        - business_objects: List of business objects
        """
        
        response = await self.llm.generate(prompt)
        return self._parse_entities(response, log)
    
    def _parse_entities(self, response: str, log: LogEntry) -> List[Entity]:
        """Parse LLM response to extract entities"""
        entities = []
        
        # Simple entity extraction (in production, use proper parsing)
        # Extract common patterns
        import re
        
        # Extract user IDs
        user_ids = re.findall(r'user[_\s]*id[:\s]*(\d+)', log.message.lower())
        for user_id in user_ids:
            entities.append(Entity(f"user_{user_id}", "user_id", user_id))
        
        # Extract error types
        error_patterns = re.findall(r'(error|exception|failure|timeout|connection)', log.message.lower())
        for error_type in error_patterns:
            entities.append(Entity(error_type, "error_type", error_type))
        
        # Extract service names
        if log.service_name:
            entities.append(Entity(log.service_name, "service_name", log.service_name))
        
        return entities
    
    async def _generate_intelligent_tags(self, log: LogEntry, entities: List[Entity]) -> List[IntelligentTag]:
        """Generate intelligent tags using LLM"""
        prompt = f"""
        Analyze this log entry and generate intelligent tags:
        
        Log: {log.message}
        Level: {log.level}
        Entities: {[e.name for e in entities]}
        
        Generate tags for:
        1. Business impact (high/medium/low)
        2. Technical category (database/api/auth/etc.)
        3. User impact (critical/important/minor)
        4. Debugging value (essential/useful/minimal)
        5. Compliance relevance (yes/no)
        """
        
        response = await self.llm.generate(prompt)
        return self._parse_intelligent_tags(response)
    
    def _parse_intelligent_tags(self, response: str) -> List[IntelligentTag]:
        """Parse LLM response to extract intelligent tags"""
        tags = []
        
        # Simple tag generation based on log level
        level_tags = {
            'ERROR': IntelligentTag('high_impact', 'business_impact', 'high'),
            'WARNING': IntelligentTag('medium_impact', 'business_impact', 'medium'),
            'INFO': IntelligentTag('low_impact', 'business_impact', 'low'),
            'DEBUG': IntelligentTag('minimal_impact', 'business_impact', 'minimal')
        }
        
        # Add level-based tag
        if hasattr(self, '_current_log'):
            level = self._current_log.level
            if level in level_tags:
                tags.append(level_tags[level])
        
        return tags
    
    async def _calculate_importance(self, log: LogEntry, entities: List[Entity]) -> float:
        """Calculate importance score for log"""
        importance = 0.5  # Base importance
        
        # Adjust based on log level
        level_scores = {
            'ERROR': 0.9,
            'WARNING': 0.7,
            'INFO': 0.5,
            'DEBUG': 0.3
        }
        
        importance = level_scores.get(log.level, 0.5)
        
        # Adjust based on entities
        if entities:
            importance += 0.1
        
        # Adjust based on message length (longer messages might be more important)
        if len(log.message) > 100:
            importance += 0.1
        
        return min(importance, 1.0)
    
    def _extract_semantic_content(self, log: LogEntry) -> str:
        """Extract semantic content from log"""
        return f"{log.level}: {log.message}"


class TemporalCorrelationEngine:
    """Analyze temporal relationships between logs"""
    
    def __init__(self):
        self.correlation_window = timedelta(minutes=5)
        self.correlation_patterns = {}
    
    async def analyze_temporal_correlations(self, logs: List[EnrichedLogEntry], 
                                          time_window: timedelta = None) -> List[CorrelationPattern]:
        """Find temporal correlations between log events"""
        
        if time_window:
            self.correlation_window = time_window
        
        correlations = []
        
        # 1. Find event sequences
        sequences = self._find_event_sequences(logs)
        
        # 2. Detect causalities
        causalities = await self._detect_causalities(sequences)
        
        # 3. Recognize patterns
        patterns = self._recognize_patterns(sequences)
        
        correlations.extend(sequences)
        correlations.extend(causalities)
        correlations.extend(patterns)
        
        return correlations
    
    def _find_event_sequences(self, logs: List[EnrichedLogEntry]) -> List[CorrelationPattern]:
        """Find sequences of related events"""
        sequences = []
        
        # Sort logs by timestamp
        sorted_logs = sorted(logs, key=lambda x: x.original_log.timestamp)
        
        # Group by time windows
        current_window = []
        window_start = None
        
        for log in sorted_logs:
            if window_start is None:
                window_start = log.original_log.timestamp
                current_window = [log]
            elif log.original_log.timestamp - window_start <= self.correlation_window:
                current_window.append(log)
            else:
                # Process current window
                if len(current_window) > 1:
                    sequence = self._create_sequence(current_window)
                    if sequence:
                        sequences.append(sequence)
                
                # Start new window
                window_start = log.original_log.timestamp
                current_window = [log]
        
        # Process final window
        if len(current_window) > 1:
            sequence = self._create_sequence(current_window)
            if sequence:
                sequences.append(sequence)
        
        return sequences
    
    def _create_sequence(self, logs: List[EnrichedLogEntry]) -> Optional[CorrelationPattern]:
        """Create a sequence pattern from logs"""
        if len(logs) < 2:
            return None
        
        # Calculate correlation strength based on log similarity
        correlation_strength = self._calculate_correlation_strength(logs)
        
        if correlation_strength < 0.3:  # Threshold for meaningful correlation
            return None
        
        # Create sequence description
        description = f"Sequence of {len(logs)} related events"
        
        return CorrelationPattern(
            pattern_type="sequence",
            events=logs,
            time_window=self.correlation_window,
            correlation_strength=correlation_strength,
            description=description,
            confidence=correlation_strength
        )
    
    def _calculate_correlation_strength(self, logs: List[EnrichedLogEntry]) -> float:
        """Calculate correlation strength between logs"""
        if len(logs) < 2:
            return 0.0
        
        # Simple correlation based on log level similarity
        levels = [log.original_log.level for log in logs]
        level_counts = {level: levels.count(level) for level in set(levels)}
        
        # Higher correlation if logs have similar levels
        max_level_count = max(level_counts.values())
        correlation_strength = max_level_count / len(logs)
        
        return correlation_strength
    
    async def _detect_causalities(self, sequences: List[CorrelationPattern]) -> List[CorrelationPattern]:
        """Detect causal relationships between events"""
        causalities = []
        
        for sequence in sequences:
            if len(sequence.events) >= 2:
                # Simple causality detection based on error patterns
                error_events = [log for log in sequence.events 
                              if log.original_log.level == 'ERROR']
                
                if error_events:
                    causality = CorrelationPattern(
                        pattern_type="causality",
                        events=sequence.events,
                        time_window=sequence.time_window,
                        correlation_strength=sequence.correlation_strength,
                        description=f"Causal relationship involving {len(error_events)} errors",
                        confidence=sequence.confidence * 0.8
                    )
                    causalities.append(causality)
        
        return causalities
    
    def _recognize_patterns(self, sequences: List[CorrelationPattern]) -> List[CorrelationPattern]:
        """Recognize common patterns in sequences"""
        patterns = []
        
        # Group sequences by pattern type
        pattern_groups = {}
        for sequence in sequences:
            pattern_type = self._classify_pattern_type(sequence)
            if pattern_type not in pattern_groups:
                pattern_groups[pattern_type] = []
            pattern_groups[pattern_type].append(sequence)
        
        # Create pattern summaries
        for pattern_type, sequences in pattern_groups.items():
            if len(sequences) > 1:  # Only create patterns with multiple instances
                pattern = CorrelationPattern(
                    pattern_type=f"recurring_{pattern_type}",
                    events=[],  # Empty for pattern summary
                    time_window=self.correlation_window,
                    correlation_strength=len(sequences) / 10.0,  # Based on frequency
                    description=f"Recurring {pattern_type} pattern ({len(sequences)} instances)",
                    confidence=min(len(sequences) / 5.0, 1.0)
                )
                patterns.append(pattern)
        
        return patterns
    
    def _classify_pattern_type(self, sequence: CorrelationPattern) -> str:
        """Classify the type of pattern in a sequence"""
        if not sequence.events:
            return "unknown"
        
        # Classify based on log levels
        levels = [log.original_log.level for log in sequence.events]
        
        if 'ERROR' in levels:
            return "error_sequence"
        elif 'WARNING' in levels:
            return "warning_sequence"
        else:
            return "info_sequence"


class LogInsightGenerator:
    """Generate actionable insights from log analysis"""
    
    def __init__(self, llm_provider: str = "openai"):
        self.llm = LLMProvider(llm_provider)
    
    async def generate_insights(self, logs: List[EnrichedLogEntry]) -> List[LogInsight]:
        """Generate intelligent insights from logs"""
        
        insights = []
        
        # 1. Anomaly insights
        anomaly_insights = await self._generate_anomaly_insights(logs)
        insights.extend(anomaly_insights)
        
        # 2. Performance insights
        performance_insights = await self._generate_performance_insights(logs)
        insights.extend(performance_insights)
        
        # 3. Business insights
        business_insights = await self._generate_business_insights(logs)
        insights.extend(business_insights)
        
        return insights
    
    async def _generate_anomaly_insights(self, logs: List[EnrichedLogEntry]) -> List[LogInsight]:
        """Generate insights about anomalies"""
        insights = []
        
        # Find error spikes
        error_logs = [log for log in logs if log.original_log.level == 'ERROR']
        
        if len(error_logs) > 0:
            # Calculate error rate
            total_logs = len(logs)
            error_rate = len(error_logs) / total_logs
            
            if error_rate > 0.1:  # More than 10% errors
                insight = LogInsight(
                    type="anomaly",
                    title="High Error Rate Detected",
                    description=f"Error rate is {error_rate:.1%} ({len(error_logs)} errors out of {total_logs} logs)",
                    severity="high",
                    actionable=True,
                    recommended_actions=[
                        "Investigate root cause of errors",
                        "Check system health metrics",
                        "Review recent deployments"
                    ],
                    confidence=min(error_rate * 2, 1.0)
                )
                insights.append(insight)
        
        return insights
    
    async def _generate_performance_insights(self, logs: List[EnrichedLogEntry]) -> List[LogInsight]:
        """Generate performance-related insights"""
        insights = []
        
        # Analyze log volume over time
        if len(logs) > 10:
            # Simple volume analysis
            timestamps = [log.original_log.timestamp for log in logs]
            time_span = max(timestamps) - min(timestamps)
            
            if time_span.total_seconds() > 0:
                logs_per_second = len(logs) / time_span.total_seconds()
                
                if logs_per_second > 100:  # High volume
                    insight = LogInsight(
                        type="performance",
                        title="High Log Volume",
                        description=f"Log volume is {logs_per_second:.1f} logs/second",
                        severity="medium",
                        actionable=True,
                        recommended_actions=[
                            "Consider log sampling optimization",
                            "Review log verbosity settings",
                            "Monitor system performance"
                        ],
                        confidence=0.7
                    )
                    insights.append(insight)
        
        return insights
    
    async def _generate_business_insights(self, logs: List[EnrichedLogEntry]) -> List[LogInsight]:
        """Generate business-related insights"""
        insights = []
        
        # Analyze service distribution
        services = [log.original_log.service_name for log in logs]
        service_counts = {service: services.count(service) for service in set(services)}
        
        if len(service_counts) > 1:
            # Find dominant service
            dominant_service = max(service_counts, key=service_counts.get)
            dominant_count = service_counts[dominant_service]
            total_count = len(services)
            
            if dominant_count / total_count > 0.7:  # More than 70% from one service
                insight = LogInsight(
                    type="business",
                    title="Service Concentration",
                    description=f"Most logs ({dominant_count}/{total_count}) are from {dominant_service}",
                    severity="low",
                    actionable=True,
                    recommended_actions=[
                        "Consider load balancing",
                        "Monitor other services",
                        "Review service architecture"
                    ],
                    confidence=0.6
                )
                insights.append(insight)
        
        return insights


class IntelligentLogAnalyzer:
    """Main analyzer that coordinates all analysis components"""
    
    def __init__(self):
        self.clusterer = SemanticLogClusterer()
        self.extractor = LogMetadataExtractor()
        self.correlation_engine = TemporalCorrelationEngine()
        self.insight_generator = LogInsightGenerator()
    
    async def analyze_logs(self, logs: List[LogEntry], analysis_types: List[str] = None,
                         time_range: timedelta = None, cluster_threshold: float = 0.7) -> Dict[str, Any]:
        """Perform comprehensive log analysis"""
        
        start_time = time.time()
        
        if analysis_types is None:
            analysis_types = ["semantic_clustering", "temporal_correlation", "insights"]
        
        # 1. Enrich logs with metadata
        enriched_logs = []
        for log in logs:
            enriched_log = await self.extractor.enrich_log(log)
            enriched_logs.append(enriched_log)
        
        results = {
            'clusters': [],
            'correlations': [],
            'insights': [],
            'metadata': {
                'total_logs': len(logs),
                'enriched_logs': len(enriched_logs),
                'analysis_types': analysis_types
            },
            'processing_time': 0
        }
        
        # 2. Perform requested analyses
        if "semantic_clustering" in analysis_types:
            clusters = await self.clusterer.cluster_logs(logs, cluster_threshold)
            results['clusters'] = clusters
        
        if "temporal_correlation" in analysis_types:
            correlations = await self.correlation_engine.analyze_temporal_correlations(
                enriched_logs, time_range
            )
            results['correlations'] = correlations
        
        if "insights" in analysis_types:
            insights = await self.insight_generator.generate_insights(enriched_logs)
            results['insights'] = insights
        
        # 3. Calculate processing time
        processing_time = time.time() - start_time
        results['processing_time'] = processing_time
        
        return results


class SemanticLogSearch:
    """Natural language search through logs"""
    
    def __init__(self, llm_provider: str = "openai"):
        self.llm = LLMProvider(llm_provider)
    
    async def search(self, query: str, logs: List[LogEntry], limit: int = 10) -> List[Dict[str, Any]]:
        """Search logs using natural language"""
        
        # 1. Generate query embedding
        query_embedding = await self.llm.generate_embeddings([query])
        
        # 2. Generate embeddings for logs
        log_texts = [f"{log.level}: {log.message}" for log in logs]
        log_embeddings = await self.llm.generate_embeddings(log_texts)
        
        # 3. Calculate similarities
        similarities = cosine_similarity(query_embedding, log_embeddings)[0]
        
        # 4. Rank and return results
        results = []
        for i, similarity in enumerate(similarities):
            if similarity > 0.1:  # Threshold for relevance
                results.append({
                    'log': logs[i],
                    'similarity': float(similarity),
                    'relevance_score': float(similarity)
                })
        
        # Sort by relevance and limit results
        results.sort(key=lambda x: x['relevance_score'], reverse=True)
        return results[:limit]
