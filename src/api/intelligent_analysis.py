"""
Intelligent Log Analysis API endpoints
"""

from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional
from uuid import UUID

from fastapi import APIRouter, Depends, HTTPException, Query
from pydantic import BaseModel, Field


class LogContext:
    """Simple log context for API"""
    
    def __init__(self, **kwargs):
        self.data = kwargs
    
    def __repr__(self):
        return f"LogContext({self.data})"

router = APIRouter(prefix="/api/v1/intelligent-analysis", tags=["intelligent-analysis"])


# Data Models
class LogEntry(BaseModel):
    """Enhanced log entry with metadata"""
    id: UUID
    timestamp: datetime
    level: str
    message: str
    service_name: str
    context: Optional[LogContext] = None
    metadata: Dict[str, Any] = Field(default_factory=dict)


class Entity(BaseModel):
    """Extracted entity from log"""
    name: str
    type: str  # user_id, service_name, error_type, resource_name, business_object
    value: str
    confidence: float = Field(ge=0.0, le=1.0)


class IntelligentTag(BaseModel):
    """Intelligent tag generated by LLM"""
    name: str
    category: str  # business_impact, technical_category, user_impact, debugging_value, compliance
    value: str
    confidence: float = Field(ge=0.0, le=1.0)


class EnrichedLogEntry(BaseModel):
    """Log entry enriched with intelligent metadata"""
    original_log: LogEntry
    entities: List[Entity] = Field(default_factory=list)
    intelligent_tags: List[IntelligentTag] = Field(default_factory=list)
    importance_score: float = Field(ge=0.0, le=1.0)
    semantic_content: str
    enrichment_metadata: Dict[str, Any] = Field(default_factory=dict)


class LogCluster(BaseModel):
    """Semantic cluster of logs"""
    id: UUID
    name: str
    description: str
    logs: List[EnrichedLogEntry]
    centroid_embedding: List[float]
    semantic_summary: str
    cluster_metadata: Dict[str, Any] = Field(default_factory=dict)


class CorrelationPattern(BaseModel):
    """Temporal correlation pattern"""
    id: UUID
    pattern_type: str  # sequence, causality, anomaly
    events: List[EnrichedLogEntry]
    time_window: timedelta
    correlation_strength: float = Field(ge=0.0, le=1.0)
    description: str
    confidence: float = Field(ge=0.0, le=1.0)


class LogInsight(BaseModel):
    """Intelligent insight generated from log analysis"""
    id: UUID
    type: str  # anomaly, performance, business, predictive
    title: str
    description: str
    severity: str  # critical, high, medium, low
    actionable: bool
    recommended_actions: List[str] = Field(default_factory=list)
    confidence: float = Field(ge=0.0, le=1.0)
    generated_at: datetime = Field(default_factory=datetime.utcnow)


class AnalysisRequest(BaseModel):
    """Request for log analysis"""
    logs: List[LogEntry]
    analysis_types: List[str] = Field(default=["semantic_clustering", "temporal_correlation", "insights"])
    time_range: Optional[timedelta] = None
    cluster_threshold: float = Field(default=0.7, ge=0.0, le=1.0)


class AnalysisResponse(BaseModel):
    """Response from log analysis"""
    clusters: List[LogCluster]
    correlations: List[CorrelationPattern]
    insights: List[LogInsight]
    analysis_metadata: Dict[str, Any] = Field(default_factory=dict)
    processing_time: float


# API Endpoints
@router.post("/analyze", response_model=AnalysisResponse)
async def analyze_logs(request: AnalysisRequest):
    """
    Perform intelligent analysis on a set of logs.
    
    This endpoint performs semantic clustering, temporal correlation analysis,
    and generates intelligent insights from the provided logs.
    """
    try:
        from src.intelligent_analysis import IntelligentLogAnalyzer
        
        analyzer = IntelligentLogAnalyzer()
        result = await analyzer.analyze_logs(
            logs=request.logs,
            analysis_types=request.analysis_types,
            time_range=request.time_range,
            cluster_threshold=request.cluster_threshold
        )
        
        return AnalysisResponse(
            clusters=result.clusters,
            correlations=result.correlations,
            insights=result.insights,
            analysis_metadata=result.metadata,
            processing_time=result.processing_time
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Analysis failed: {str(e)}")


@router.post("/enrich", response_model=List[EnrichedLogEntry])
async def enrich_logs(logs: List[LogEntry]):
    """
    Enrich logs with intelligent metadata and entities.
    """
    try:
        from src.intelligent_analysis import LogMetadataExtractor
        
        extractor = LogMetadataExtractor()
        enriched_logs = []
        
        for log in logs:
            enriched_log = await extractor.enrich_log(log)
            enriched_logs.append(enriched_log)
        
        return enriched_logs
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Enrichment failed: {str(e)}")


@router.post("/cluster", response_model=List[LogCluster])
async def cluster_logs(
    logs: List[LogEntry],
    threshold: float = Query(default=0.7, ge=0.0, le=1.0)
):
    """
    Perform semantic clustering on logs.
    """
    try:
        from src.intelligent_analysis import SemanticLogClusterer
        
        clusterer = SemanticLogClusterer()
        clusters = await clusterer.cluster_logs(logs, threshold=threshold)
        
        return clusters
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Clustering failed: {str(e)}")


@router.post("/correlate", response_model=List[CorrelationPattern])
async def correlate_logs(
    logs: List[EnrichedLogEntry],
    time_window: timedelta = Query(default=timedelta(minutes=5))
):
    """
    Find temporal correlations between logs.
    """
    try:
        from src.intelligent_analysis import TemporalCorrelationEngine
        
        engine = TemporalCorrelationEngine()
        correlations = await engine.analyze_temporal_correlations(logs, time_window)
        
        return correlations
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Correlation analysis failed: {str(e)}")


@router.post("/insights", response_model=List[LogInsight])
async def generate_insights(logs: List[EnrichedLogEntry]):
    """
    Generate intelligent insights from logs.
    """
    try:
        from src.intelligent_analysis import LogInsightGenerator
        
        generator = LogInsightGenerator()
        insights = await generator.generate_insights(logs)
        
        return insights
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Insight generation failed: {str(e)}")


@router.get("/search")
async def semantic_search(
    query: str = Query(..., description="Natural language search query"),
    logs: List[LogEntry] = Depends(),  # This would need to be implemented as a dependency
    limit: int = Query(default=10, ge=1, le=100)
):
    """
    Search logs using natural language queries.
    """
    try:
        from src.intelligent_analysis import SemanticLogSearch
        
        searcher = SemanticLogSearch()
        results = await searcher.search(query, logs, limit=limit)
        
        return {"query": query, "results": results}
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Search failed: {str(e)}")


@router.get("/health")
async def health_check():
    """Health check for intelligent analysis service"""
    return {
        "status": "healthy",
        "service": "intelligent-analysis",
        "timestamp": datetime.utcnow()
    }
